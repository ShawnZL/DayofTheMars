# 20221025 DQN

## Q-Learning

**这篇文章将要介绍传统的qlearning算法，使用的是迭代的方法更新q表，更新q表的方法类似于向前推进，而不是使用梯度下降方法，因为这里介绍的不是Deep QLearning方法。**

### 1.1 Q(s,a)

Q(s,a)是状态动作价值函数，是在状态s 时采取动作a 之后，可以获得的奖励的期望值。Q (s, a)越大表示在agent在看到状态s 是采取动作a 比较好。

### 1.2 q_table

|      | a1       | a2       | a3       |
| ---- | -------- | -------- | -------- |
| s1   | q(s1,a1) | q(s1,a2) | q(s1,a3) |
| s2   | q(s2,a1) | q(s2,a2) | q(s2,a3) |
| s3   | q(s3,a1) | q(s3,a2) | q(s3,a3) |
| s4   | q(s4,a1) | q(s4,a2) | q(s4,a3) |

q表里面记录的都是状态动作价值函数，前面说到q表可以间接决定agent采取什么样的决策，就是因为q表记录了所有的状态和动作的组合情况，比如agent看到状态s2时，就会在状态s2所在的行选取最大的q值所对应的动作。

### 1.3 如何利用q-table决策

**假设下表是我们已经更新完成了的q表**

| q表  | a1   | a2   | a3   |
| ---- | ---- | ---- | ---- |
| s1   | - 1  | 1    | 3    |
| s2   | 2    | 0    | 1    |
| s3   | 1    | 5    | 7    |
| s4   | 5    | 6    | 3    |

Q表指导agent决策的过程：t=1时，agent观测到环境的状态s2，于是查找状态s2所在的行，发现Q(s2,a1)>Q(s2,a3)>Q(s2,a2),因此选择动a1,此时环境发生变化，agent观测到环境的状态s4，接着查找状态s4所在的行，agent发现q(s4,a2)>q(s4,a1)>q(s4,a3)，于是agent采取决策选择动作a2，一直进行下去，直到结束。

### 1.4 ε−greedy选择动作

有上面的决策过程我们可以看到，在给定一个状态s时，会选择q值最大的动作，这样会导致一个问题：因为是随机初始化的，由于选择最大值，可能会使得一些动作无法被选择到，也就是无法更新q值，这样q值就一直是随机初始化的那个值。

ε − greedy 选择动作的流程如下：agent观测到状态s时，采取动作时会以1 − ε 的概率在Q表里面选择q值最大所对应的动作，以ε的概率随机选择动作。
不再使用完全贪婪的算法，而是有一定的动作选择的完全随机性，这样就可以保证在迭代次数足够多的情况下Q表中的所有动作都会被更新到。

取0.9就是90的可能性选择最优解，0.1可能性随机选择

### 1.5 跟新q表

![](https://img-blog.csdn.net/20180615180722209?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

`r` reward奖励值， `s'` `s`做出行动后的下一个状态，`α` 学习效率表示有多少误差要被学习，`γ` 未来奖励的衰减值
$$
Q(s,a) = Q(s,a)+α[r + γmax_{a′}Q(s′,a′)-Q(s,a)]
$$

#### 推理过程

|      | a1   | a2   |
| ---- | ---- | ---- |
| s1   | -2   | 1    |
| s2   | -4   | 2    |

在`s1`选择a2行动，在到达`s2`后，更新q_table，然后在`s2`时并没实际行动，Q(s2,a1) < Q(s2,a2)

```
Q(s1,a2)现实 = reward + γ*maxQ(s2)
Q(s1,a2)估计 = Q(s1,a2) # 是直接根据表格估计s1,a2的值

差距 = 现实-估计
新Q(s1,a2) = 老Q(s1,a2) + α*差距 # Q值更新公式 
```


$$
Q(s1) = r2+γQ(s2) = r2 + γ[r3+γQ(s3)]=......
$$

$$
Q(s1)=r2+γr3+γ^2r4+γ^3r5+γ^4r6+......
$$



虽然使用Q(s2,a2)来更新上一个状态，但是并没有在状态`s2`进行选择采取行动，`s2`的行为决策应该等到更新完毕后再做重新另外做。

1.α 指学习率，其实也是一个权值，我们将公式1进行改写得到如下的公式
$$
Q(s,a) 
 =(1−α)Q(s,a)+α[r+γmaxa′Q(s ′,a ′)]
$$
2`γ`衰减值

首先我们随机初始化一个Q表，然后任意初始化一个状态s，也可以理解为，agent观测到的环境的状态，根据Q表使用ε−greedy算法选择状态s对应的动作a,因为agent做出了一个动作，会从环境中获得一个奖励 r ，环境发生变化，agent又观测到一个新的状态 s ′，根据Q表在状态 s ′所在行，查询Q表，求得最大值，然后更新按照公式更新Q表

可以看到，Q ( s , a ) 的更新会使用到状态s ′的Q值，从而基于TD方法进行更新Q表。但是每次更新我们都利用了下一个时刻的Q值，比如Q ( s ′ , a ′ )，假设agent完了一个episode的游戏，得到τ = { s1 , a1 , r1 , s2 , a2 , r2 , . . . , sT , aT , rT , END} ,假如我们先从后面的状态开始更新，这样前面的状态更新的时候就可以使用已经更新了的后面的状态，也就是反向更新
不知道这样是否可行，这样有个问题就是：原始的方法每一时间步都可以更新，就是在episode进行的时候更新，换成上面我说的这样的更新方法，还要将状态，动作，价值先存储起来，要等到episode结束才能更新。

可以看见Q-Learning是属于值函数近似算法中，蒙特卡洛方法和时间差分法相结合的算法

[参考链接1](https://blog.csdn.net/qq_41626059/article/details/114364666?spm=1001.2101.3001.6650.5&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-114364666-blog-9361915.pc_relevant_3mothn_strategy_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-114364666-blog-9361915.pc_relevant_3mothn_strategy_recovery&utm_relevant_index=6)

## DQN

纬度灾难，在上面的简单分析中，我们使用表格来表示Q(s,a)，但是这个在现实的很多问题上是几乎不可行的，因为状态实在是太多。使用表格的方式根本存不下。

![](https://pic2.zhimg.com/80/e7df063e461eef1079943eceda1c3e2d_1440w.webp)

计算机玩Atari游戏的要求是输入原始图像数据，也就是210x160像素的图片，然后输出几个按键动作。总之就是和人类的要求一样，纯视觉输入，然后让计算机自己玩游戏。那么这种情况下，到底有多少种状态呢？有可能每一秒钟的状态都不一样。因为，从理论上看，如果每一个像素都有256种选择，`256*210*160`

### 价值函数近似Value Function Approximation

什么是价值函数近似呢？说起来很简单，就是用一个函数来表示Q(s,a)。
$$
Q(s,a) = f(s,a)
$$
f可以是任意类型的函数，比如线性函数：
$$
Q(s,a)=w_1s+w_2s+b
$$
通过函数表示，我们就可以无所谓s到底是多大的维度，反正最后都通过矩阵运算降维输出为单值的Q

如果我们就用w来统一表示函数f的参数，那么就有
$$
Q(s,a)=f(s,a,w)
$$
为什么叫近似，因为我们并不知道Q值的实际分布情况，本质上就是用一个函数来近似Q值的分布，所以，也可以说是
$$
Q(s,a)≈f(s,a,w)
$$

### 高维状态输入，低维状态输出

对于Atari游戏而言，这是一个高维状态输入（原始图像），低维动作输出（只有几个离散的动作，比如上下左右）。那么怎么来表示这个函数f呢？

难道把高维s和低维a加在一起作为输入吗？

必须承认这样也是可以的。但总感觉有点别扭。特别是，其实我们只需要对高维状态进行降维，而不需要对动作也进行降维处理。

其实就是Q(s)≈f(s,w)，只把状态s作为输入，但是输出的时候输出每一个动作的Q值，也就是输出一个向量[Q(s,a1),Q(s,a2),Q(s,a3),...,Q(s,an)]，记住这里输出是一个值，只不过是包含了所有动作的Q值的向量而已。这样我们就只要输入状态s，而且还同时可以得到所有的动作Q值，也将更方便的进行Q-Learning中动作的选择与Q值更新



如果要在连续的状态空间中计算Q函数𝑄𝜋(𝑠,𝑎)，使用Q表是不现实的选择，因为连续的状态空间会让Q表的存储空间变得非常大。那么我们可以用一个关于s和a的连续函数𝑄𝜃(𝑠,𝑎)去近似𝑄𝜋(𝑠,𝑎)。𝑄𝜃(𝑠,𝑎) 是一个参数为𝜃的函数，近似这种事就是神经网络的老本行。

不管𝑄𝜋(𝑠,𝑎)是怎样一个复杂的函数，只要使用Deep Network，就有机会去近似它。

如果动作空间A是离散的,共有𝑚个动作，𝑎1,…,𝑎𝑚那么我们可以定义𝑄𝜃是状态𝑠的函数，输出是𝑚个向量：
$$
𝑄𝜃(𝑠)=\begin{bmatrix}
   𝑄𝜃(𝑠,𝑎1)\\
   ⋮ \\
   𝑄𝜃(𝑠,𝑎𝑚) 
  \end{bmatrix}≈\begin{bmatrix}
   𝑄𝜋(𝑠,𝑎1)\\
   ⋮ \\
   𝑄𝜋(𝑠,𝑎𝑚)
  \end{bmatrix} \tag{6}
$$
神经网络接受状态向量𝑠为输入，然后输出的m个向量分别作为真实𝑄𝜋的每一维的近似。



### Loss function

神经网络的训练是一个最优化问题，最优化一个损失函数loss function，也就是标签和网络输出的偏差，目标是让损失函数最小化。

大家回想一下Q-Learning算法，Q值的更新依靠什么？依靠的是利用Reward和Q计算出来的目标Q值：
$$
R_{t+1}+𝛾max_aQ(s+1,a)
$$
因此，我们把目标Q值作为标签不就完了？我们的目标不就是让Q值趋近于目标Q值吗？

已经确定要使用神经网络近似，我们拿什么作为损失函数来学习它的参数 𝜃 呢？既然我们的目标函数𝑄𝜋输出的是连续的数值，就先用平方损失函数写出来
$$
𝐿𝑖(𝜃𝑖)=𝐸_{𝑠,𝑎∼𝜌(⋅)}[(𝑦𝑖−𝑄(𝑠,𝑎;𝜃𝑖))^2]
$$

$$
L(w) = E[(r+𝛾max_{a'}Q(s',a',w)-Q(s,a,w))^2]
$$

𝐿𝑖(𝜃𝑖)表示的第𝑖次迭代的损失函数。仔细对比Q-Learning的定义，损失函数中的样本真实目标值𝑦𝑖可以写成
$$
𝑦𝑖=𝐸_{𝑠′∼𝐸}[𝑟+𝛾max_{𝑎′}𝑄(𝑠′,𝑎′;𝜃𝑖−1)|𝑠,𝑎]
$$
这就是当前时刻状态𝑠和动作𝑎确定的情况下，执行动作𝑎，环境𝐸会进入下一个时刻的状态𝑠′；然后在𝑠′状态下，计算这个动作最优动作价值函数，也就是Q现实

![](https://upload-images.jianshu.io/upload_images/26635760-b956df288f77f9c5.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

应用了经验回放（Experience Replay)机制，用一个存储空间来存储最近N次的记录，![\phi](https://math.jianshu.com/math?formula=%5Cphi)是状态的函数。与Q-learning 对比，关键点在于创建了记忆库Memory D。实际项目中，我们会先让记忆库积累到一定的数量，才会开始抽取样本，训练神经网络，梯度下降去优化。

- 首先初始化Memory D，它的容量为N;

- 初始化Q网络，随机生成权重ω;

- 循环遍历episode =1, 2, …, M:

- - 初始化initial state S1;

  - 循环遍历step =1,2,…, T:

  - - \1) 用 ϵ−greedy 策略生成action at ；
    - \2) 执行action at ，接受reward rt 及新的state St+1 ；
    - \3) 将transition样本（ St,at,rt,St+1 ）存入D中；
    - \4) 令 yj=rj ，如果j+1步是terminal的话，否则，令 yj=rj+γ∗maxQ(St+1,a)
    - \5) 对 (yj−Q(St,aj,w))2 关于 w 使用梯度下降法进行更新，根据方程式3

  - end for

- end for

![](https://pic3.zhimg.com/80/v2-0fcdb5a23bd3b193272dd79ec43d91d2_1440w.webp)

![](https://pic2.zhimg.com/v2-5716a136b9653f205ec362c759ca44f9_1440w.jpg?source=172ae18b)



### 参考链接

[链接一](https://qddmj.cn/dqn.htm)

[链接二](https://zhuanlan.zhihu.com/p/21421729)

## 历史解读

**一个基本的MDP可以用（S,A,P）来表示，S表示状态，A表示动作，P表示状态转移概率，也就是根据当前的状态st和at转移到st+1的概率**。如果我们知道了转移概率P，也就是称为我们获得了**模型Model**，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为Model-based的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有Model-free的方法来寻找最优的动作。关于具体的方法这里不具体讨论。在以后的文章中我们会通过分析具体的算法对此有个明确的认识。

### 回报Result

既然一个状态对应一个动作，或者动作的概率，而有了动作，下一个状态也就确定了。这就意味着每个状态可以用一个确定的值来进行描述。可以由此判断一个状态是好的状态还是不好的状态。比如，向左边走就是悬崖，悬崖肯定不是好的状态，再走一步可能就挂了，而向右走就是黄金，那么右边的状态就是好的状态。

那么状态的好坏其实等价于对未来回报的期望。因此，引入**回报Return**来表示某个时刻t的状态将具备的回报：
$$
G_t=R_{t+1}+λR_{t+2}+...=∑^{k=0}_∞λkR_{t+k+1}
$$
上面R是Reward反馈，λ是discount factor折扣因子，一般小于1，就是说一般当下的反馈是比较重要的，时间越久，影响越小。

那么实际上除非整个过程结束，否则显然我们无法获取所有的reward来计算出每个状态的Return，因此，再引入一个概念价值函数Value Function,用value function v(s)来表示一个状态未来的潜在价值。还是上面的例子，这里就变成是向左看感觉左边是悬崖那么左边的状态的估值就低。从定义上看，value function就是回报的期望：
$$
v(s) = E[G_t|S_t=s]
$$
引出价值函数，对于获取最优的策略Policy这个目标，我们就会有两种方法：

- 直接优化策略π(a|s)或者a=π(s)使得回报更高
- 通过估计value function来间接获得优化的策略。道理很简单，既然我知道每一种状态的优劣，那么我就知道我应该怎么选择了，而这种选择就是我们想要的策略。

当然了，还有第三种做法就是融合上面的两种做法，这也就是以后会讲到的actor-critic算法。但是现在为了理解DQN，我们将只关注第二种做法，就是估计value function的做法，因为DQN就是基于value function的算法。

MDP只需要用一句话就可以说明白，就是“未来只取决于当前”，专业点说就是下一步的状态只取决于当前的状态，与过去的状态没有关系。

在引出了MDP之后，由于每一个时刻的状态是确定的，因此我们可以用Value Function价值函数来描述这个状态的价值，从而确定我们的决策方式。

**从数学的角度，我们常常会用一个函数V(s)来表示一个状态的价值，也可以用Q(s,a)来表示状态及某一个动作的价值。我们上面的例子就是来评估某一个状态下动作的价值，然后根据价值做出判断。实际上我们这里也是有策略的，我们的策略更简单：**

```text
if 某一个决策的价值最大：
    选择这个决策
```

### Bellman方程

在上文我们介绍了Value Function价值函数，所以为了解决增强学习的问题，一个显而易见的做法就是----

> 我们需要估算Value Function

是的，只要我们能够计算出价值函数，那么最优决策也就得到了。因此，问题就变成了如何计算Value Function？

**回报Result**的基本定义吗？就是所有Reward的累加（带衰减系数discount factor）
$$
G_t=R_{t+1}+λR_{t+2}+...=∑^{k=0}_∞λkR_{t+k+1}
$$
那么Value Function该如何定义？也很简单，就是期望的回报啊！期望的回报越高，价值显然也就越大，也就越值得去选择。用数学来定义就是如下：

那么Value Function该如何定义？也很简单，就是期望的回报啊！期望的回报越高，价值显然也就越大，也就越值得去选择。用数学来定义就是如下：
$$
v(s)=E[G_t|S_t=s]
$$
接下来，我们把上式展开如下：
$$
\begin{aligned}
a &= E[G_t|S_t=s] \\
  &= E[R_{t+1}+λR_{t+2}+λ^2R_{t+3}+...|S_t=s] \\
  &= E[R_{t+1}+λ(R_{t+2}+λR_{t+3}+...)|S_t=s] \\
  &= E[R_{t+1}+λG_{t+1}|S_t=s] \\
  &= E[R_{t+1}+λv(S_{t+1})|S_t=s]
\end{aligned}
$$
`v(St+1)` 是下一个状态t+1时的value

上面这个公式就是**Bellman方程**的基本形态。从公式上看，当前状态的价值和下一步的价值以及当前的反馈Reward有关。
